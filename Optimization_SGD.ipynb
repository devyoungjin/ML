{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimization SGD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyODlcY3g0yDjT+OroxhWNmM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devyoungjin/ML/blob/main/Optimization_SGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimization: Stochastic Gradient Descent\n",
        "1. Batch Gradient Descent\n",
        "2. Stochastic Gradient Descent\n",
        "3. Mini-batch Gradient Descent"
      ],
      "metadata": {
        "id": "Zb2S7AmKwzWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\omega \\leftarrow $\n",
        "$\\omega - α \\nabla f{(\\omega)}$\n",
        "\n",
        "from some step size (or learning rate) $α$>0\n",
        "\n",
        "$ {\\displaystyle \\nabla =\\sum _{i=1}^{n}\\mathbf {e} _{i}{\\frac {\\partial }{\\partial x_{i}}}}$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hjP1JKGSmQ_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.SGD \n",
        "매 이터레이션 마다 전체 데이터를 배치로 가지고와서 그레디언트 계산하는 것이 아니라 오메가를 업데이트함에 있어서 randomly selected single training\n",
        "SGD: update the parameters based on the gradient for a randomly selected single training\n",
        "SGD takes steps in a noisy directon, but moves downhill on average.\n",
        "계산량이 아주 적음 (하나의 데이터에 대한 expectation이 전체를 대변할 수 있다면 좋음)"
      ],
      "metadata": {
        "id": "un0LewIpxdr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#3. Mini-batch\n",
        "Potential problem of SGD: Gradient estimates can be very noisy"
      ],
      "metadata": {
        "id": "PJqutigCx3BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BATCH\n",
        "LR = 0.04\n",
        "n_iter =10000\n",
        "n_prt= 250\n",
        "\n",
        "x = tf.placeholder(tf.float32,[m,1])\n",
        "y = tf.placeholder(tf.float32,[m,1])\n",
        "\n",
        "w = tf.Variable([[0],[0],[0]],dtype=tf.float32)\n",
        "\n",
        "y_pred = tf.matmul(x,w)\n",
        "loss =tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred,labels=y)\n",
        "loss =tf.reduce_mean(loss) #sclar\n",
        "\n",
        "optm = tf.train.GradientDescentOptimizer(LR).minimize(loss)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "loss_record=[]\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for epoch in range(n_iter):\n",
        "    sess.run(optm,feed_dict={x:train_x, y:train_y})\n",
        "\n",
        "    if (epoch +1) % n_prt ==0:\n",
        "      loss_record.append(sess.run(loss,feed_dict={x:train_X, y:train_y}))\n",
        "\n",
        "  w_hat =sess.run(w)\n",
        "sess.close()"
      ],
      "metadata": {
        "id": "dXGNvCl4qUKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SGD\n",
        "LR = 0.04\n",
        "n_iter =10000\n",
        "n_prt= 250\n",
        "\n",
        "x = tf.placeholder(tf.float32,[m,1])\n",
        "y = tf.placeholder(tf.float32,[m,1])\n",
        "\n",
        "w = tf.Variable([[0],[0],[0]],dtype=tf.float32)\n",
        "\n",
        "y_pred = tf.matmul(x,w)\n",
        "loss =tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred,labels=y)\n",
        "loss =tf.reduce_mean(loss) #sclar\n",
        "\n",
        "optm = tf.train.GradientDescentOptimizer(LR).minimize(loss)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "loss_record=[]\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for epoch in range(n_iter):\n",
        "    idx =np.random.choice(m,1) #1 sample\n",
        "    batch_X = train_X[idx,:]\n",
        "    batch_y = train_y[idx] \n",
        "    sess.run(optm,feed_dict={x:train_X, y:train_y})\n",
        "\n",
        "    if (epoch +1) % n_prt ==0:\n",
        "      loss_record.append(sess.run(loss,feed_dict={x:train_X, y:train_y}))\n",
        "\n",
        "  w_hat =sess.run(w)\n",
        "sess.close()"
      ],
      "metadata": {
        "id": "AKzt1AKqKde2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MINI-Batch\n",
        "LR = 0.04\n",
        "n_iter =10000\n",
        "n_prt= 250\n",
        "n_batch = 50 #batch_size\n",
        "x = tf.placeholder(tf.float32,[m,1])\n",
        "y = tf.placeholder(tf.float32,[m,1])\n",
        "\n",
        "w = tf.Variable([[0],[0],[0]],dtype=tf.float32)\n",
        "\n",
        "y_pred = tf.matmul(x,w)\n",
        "loss =tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred,labels=y)\n",
        "loss =tf.reduce_mean(loss) #sclar\n",
        "\n",
        "optm = tf.train.GradientDescentOptimizer(LR).minimize(loss)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "loss_record=[]\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for epoch in range(n_iter):\n",
        "    idx =np.random.choice(m,size=n_batch) #size 50\n",
        "    batch_X = train_X[idx,:]\n",
        "    batch_y = train_y[idx] \n",
        "    sess.run(optm,feed_dict={x:train_X, y:train_y})\n",
        "\n",
        "    if (epoch +1) % n_prt ==0:\n",
        "      loss_record.append(sess.run(loss,feed_dict={x:train_X, y:train_y}))\n",
        "\n",
        "  w_hat =sess.run(w)\n",
        "sess.close()"
      ],
      "metadata": {
        "id": "rN8UwAwlLDWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limitation of the Gradient Descent"
      ],
      "metadata": {
        "id": "oBTlXs41MBeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.공간적 issue\n",
        "#2.시간적 issue\n",
        "\n",
        "# Adaptive Learning Rate Methods \n",
        "# Momentum / Adagrad / Adadelta / Adam / PMSProp\n",
        "# 특징과 장단점 찾아보기"
      ],
      "metadata": {
        "id": "0dJjGOW2MDoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Nd3rs1ENQCEA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}